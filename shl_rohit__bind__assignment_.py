# -*- coding: utf-8 -*-
"""SHL_Rohit _bind _Assignment .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ueGH_ZWAfYlOjZnor8CfAozVElBvljEV
"""

# Install required libraries
!pip install -q transformers torch torchaudio
!pip install -q openai-whisper
!pip install -q language-tool-python
!pip install -q spacy
!pip install -q sentence-transformers
!pip install -q lightgbm
!pip install -q pandas numpy scikit-learn
!pip install -q datasets
!pip install xgboost==1.7.6 scikit-learn==1.3.2
!pip install -q language-tool-python

!apt-get update
!apt-get install -y openjdk-17-jre-headless

# Download spaCy model
!python -m spacy download en_core_web_sm

## üì¶ Import Libraries

import os
import pandas as pd
import numpy as np
import torch
import torchaudio
import whisper
from pathlib import Path
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')
# Grammar checking
import language_tool_python
# NLP processing
import spacy
from collections import Counter
# Embeddings
from sentence_transformers import SentenceTransformer

# ML models
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import lightgbm as lgb
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

"""# File_Path"""

# Define paths
TRAIN_AUDIO_PATH = "/content/drive/MyDrive/dataset/audios/train"
TEST_AUDIO_PATH = "/content/drive/MyDrive/dataset/audios/test"
TRAIN_CSV_PATH = "/content/drive/MyDrive/dataset/csvs/train.csv"
TEST_AUDIO_CSV="/content/drive/MyDrive/dataset/csvs/test.csv"

print(TEST_AUDIO_PATH)

#Load training data
train_df = pd.read_csv(TRAIN_CSV_PATH)
print(f"Training samples: {len(train_df)}")
print(train_df.head())

# Load Whisper model
print("Loading Whisper model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
whisper_model = whisper.load_model("large-v3", device=device)  # Use 'large-v3' for better accuracy
print(f"‚úÖ Whisper model loaded on {device}")

"""# Whisper ASR Transcription"""

def transcribe_audio(audio_path):
    """Transcribe audio file to text using Whisper"""
    try:
        result = whisper_model.transcribe(
            audio_path,
            language="en",
            task="transcribe",
            fp16=(device == "cuda")
        )
        return result["text"].strip()
    except Exception as e:
        print(f"Error transcribing {audio_path}: {e}")
        return ""

#Transcribe all training audios
print("Transcribing training audios...")
transcripts = []

for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):
    audio_file = row['filename']
    audio_path = os.path.join(TRAIN_AUDIO_PATH, row["filename"] + ".wav")
  # Adjust column name
    if os.path.exists(audio_path):
        transcript = transcribe_audio(audio_path)
        transcripts.append(transcript)
    else:
        print(f"‚ö†Ô∏è Audio not found: {audio_path}")
        transcripts.append("")

train_df['transcript'] = transcripts
# Save transcripts for future use
train_df.to_csv('/content/train_with_transcripts.csv', index=False)
print("‚úÖ Transcription completed and saved")

train_df=pd.read_csv("/content/train_with_transcripts.csv")

train_df.head()

"""# Feature Extraction"""

### **2.1 Grammar Error Detection**
import language_tool_python

# Initialize LanguageTool (grammar checker)
print("Initializing grammar checker...")
tool = language_tool_python.LanguageTool('en-US')

def extract_grammar_features(text):
    """Extract grammar error features using LanguageTool"""
    if not text or len(text.strip()) == 0:
        return {
            'num_errors': 0,
            'error_density': 0,
            'spelling_errors': 0,
            'grammar_errors': 0,
            'style_errors': 0
        }

    matches = tool.check(text)

    # Count different error types
    error_types = Counter([m.ruleId for m in matches])
    spelling_errors = sum(1 for m in matches if 'SPELL' in m.ruleId or 'MORFOLOGIK' in m.ruleId)
    grammar_errors = sum(1 for m in matches if 'GRAMMAR' in m.ruleId or any(x in m.ruleId for x in ['VERB', 'AGREEMENT', 'TENSE']))
    style_errors = len(matches) - spelling_errors - grammar_errors

    word_count = len(text.split())

    return {
        'num_errors': len(matches),
        'error_density': len(matches) / word_count if word_count > 0 else 0,
        'spelling_errors': spelling_errors,
        'grammar_errors': grammar_errors,
        'style_errors': style_errors
    }

print("‚úÖ Grammar checker initialized")

### **2.2 Linguistic Features (spaCy)**

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

def extract_linguistic_features(text):
    """Extract linguistic features using spaCy"""
    if not text or len(text.strip()) == 0:
        return {
            'word_count': 0,
            'sentence_count': 0,
            'avg_sentence_length': 0,
            'vocab_richness': 0,
            'noun_ratio': 0,
            'verb_ratio': 0,
            'adj_ratio': 0,
            'adv_ratio': 0,
            'avg_dep_depth': 0,
            'passive_voice_ratio': 0
        }

    doc = nlp(text)

    # Basic counts
    word_count = len([token for token in doc if not token.is_punct])
    sentences = list(doc.sents)
    sentence_count = len(sentences)

    # Sentence length
    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0

    # Vocabulary richness (Type-Token Ratio)
    unique_words = len(set([token.lemma_.lower() for token in doc if token.is_alpha]))
    vocab_richness = unique_words / word_count if word_count > 0 else 0

    # POS tag distribution
    pos_counts = Counter([token.pos_ for token in doc])
    noun_ratio = pos_counts['NOUN'] / word_count if word_count > 0 else 0
    verb_ratio = pos_counts['VERB'] / word_count if word_count > 0 else 0
    adj_ratio = pos_counts['ADJ'] / word_count if word_count > 0 else 0
    adv_ratio = pos_counts['ADV'] / word_count if word_count > 0 else 0

    # Dependency parsing depth
    dep_depths = [len(list(token.ancestors)) for token in doc]
    avg_dep_depth = np.mean(dep_depths) if dep_depths else 0

    # Passive voice detection
    passive_count = sum(1 for token in doc if token.dep_ == 'nsubjpass')
    passive_voice_ratio = passive_count / sentence_count if sentence_count > 0 else 0

    return {
        'word_count': word_count,
        'sentence_count': sentence_count,
        'avg_sentence_length': avg_sentence_length,
        'vocab_richness': vocab_richness,
        'noun_ratio': noun_ratio,
        'verb_ratio': verb_ratio,
        'adj_ratio': adj_ratio,
        'adv_ratio': adv_ratio,
        'avg_dep_depth': avg_dep_depth,
        'passive_voice_ratio': passive_voice_ratio
    }

print("‚úÖ Linguistic feature extractor ready")

### **2.3 Semantic Embeddings (SBERT)**

# Load Sentence-BERT model
print("Loading Sentence-BERT model...")
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')

def extract_embeddings(text):
    """Extract semantic embeddings using Sentence-BERT"""
    if not text or len(text.strip()) == 0:
        return np.zeros(384)  # all-MiniLM-L6-v2 produces 384-dim embeddings

    embedding = sbert_model.encode(text, convert_to_numpy=True)
    return embedding

print("‚úÖ Sentence-BERT model loaded")

### **2.4 Combined Feature Extraction**
def extract_all_features(text):
    """Extract all features from transcript"""
    # Grammar features
    grammar_feats = extract_grammar_features(text)

    # Linguistic features
    linguistic_feats = extract_linguistic_features(text)

    # Semantic embeddings
    embeddings = extract_embeddings(text)

    # Combine all features
    all_features = {**grammar_feats, **linguistic_feats}

    return all_features, embeddings

# Extract features for all training samples
print("Extracting features from transcripts...")
feature_list = []
embeddings_list = []

for transcript in tqdm(train_df['transcript']):
    features, embeddings = extract_all_features(transcript)
    feature_list.append(features)
    embeddings_list.append(embeddings)

len(feature_list)

# Convert to DataFrame
features_df = pd.DataFrame(feature_list)
embeddings_array = np.vstack(embeddings_list)

print(f"‚úÖ Feature extraction completed")
print(f"   Structured features shape: {features_df.shape}")
print(f"   Embeddings shape: {embeddings_array.shape}")

print(features_df.head(1))
type(features_df)

train_df.columns

""" **Scaling & Train/Val Split**"""

## ü§ñ Stage 3: Model Training

### **3.1 Prepare Training Data**

# Combine structured features and embeddings
X_structured = features_df.values
X_embeddings = embeddings_array
X_combined = np.hstack([X_structured, X_embeddings])

# Target variable (grammar score)
y = train_df['label'].values # Adjust column name

X_combined

y[0:3]

# Split data
X_train, X_val, y_train, y_val = train_test_split(
    X_combined, y, test_size=0.2, random_state=42
)

import numpy as np
# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

print(f"Training set: {X_train.shape}")
print(f"Validation set: {X_val.shape}")
# print("Means:", scaler.mean_)       # Mean for each feature
# print("Std devs:", scaler.scale_)
print(X_train_scaled.shape)

len(X_train_scaled)
len(y_train)

len(y_train)

len(train_df)

"""**ML Model Training**"""

from sklearn.model_selection import GridSearchCV
import xgboost as xgb
import time

param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 0.1],
    'min_child_weight': [1, 3, 5]
}

grid_search = GridSearchCV(
    estimator=xgb.XGBRegressor(tree_method="gpu_hist", verbosity=2),
    param_grid=param_grid,
    scoring='neg_root_mean_squared_error',
    cv=3,
    verbose=5
)

start_time = time.time()
grid_search.fit(X_train_scaled, y_train)
end_time = time.time()

print('Best params:', grid_search.best_params_)
print('Best score:', grid_search.best_score_)
print('Total training time (seconds):', end_time - start_time)

best_model = grid_search.best_estimator_

import matplotlib.pyplot as plt

# Get the underlying Booster object
booster = grid_search.best_estimator_.get_booster()

# Plot feature importance using Booster
xgb.plot_importance(booster)
plt.show()

import matplotlib.pyplot as plt
import xgboost as xgb

# For XGBoost, get the Booster object from best estimator:
booster = grid_search.best_estimator_.get_booster()

# Plot only top 20 features, set figure size for clarity
xgb.plot_importance(booster, max_num_features=20, height=0.6)
plt.title("Top 20 XGBoost Feature Importances")
plt.tight_layout()
plt.show()

print(best_model)

import joblib
#joblib.dump(best_model, "best_xgboost_sklearn.pkl")
# Later:
best_model = joblib.load("best_xgboost_sklearn.pkl")

# Predict on the validation set
y_pred_xgb = best_model.predict(X_val_scaled)

import seaborn as sns
residuals = y_val - y_pred_xgb
sns.histplot(residuals)
plt.title("Validation Residuals Distribution")
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(7,7))

# True scores (as diagonal, ideal predictions)
plt.scatter(y_val, y_val, color='green', alpha=0.5, label='True Scores')

# Predicted scores (actual model predictions)
plt.scatter(y_val, y_pred_xgb, color='red', alpha=0.5, label='Predicted Scores')

plt.xlabel("True Scores")
plt.ylabel("Scores")
plt.title("True (Green Diagonal) vs. Predicted (Red) Grammar Scores (Validation)")
plt.legend()
plt.show()

"""

# Xgboost RMSE"""

# Compare predictions to actual y_val labels
from sklearn.metrics import mean_squared_error
import numpy as np

val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_xgb))
print('Validation RMSE:', val_rmse)

print(y_val.min(), y_val.max(), y_val.mean(), y_val.std())

from sklearn.metrics import mean_squared_error
import numpy as np
baseline_rmse = np.sqrt(mean_squared_error(y_val, [y_train.mean()] * len(y_val)))
print('Baseline RMSE:', baseline_rmse)

#traininglightGBM model
from sklearn.model_selection import GridSearchCV
import lightgbm as lgb
import time

# Parameter grid for LightGBM
param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'num_leaves': [31, 63],
    'min_child_samples': [10, 20],
    'reg_alpha': [0.0, 0.1],
    'reg_lambda': [0.0, 1.0]
}

# Make sure LightGBM is set for GPU
lgb_estimator = lgb.LGBMRegressor(
    n_estimators=200,
    random_state=42,
    device='gpu',       # Train on GPU
    verbosity=2
)

# Grid Search setup
grid_search_1= GridSearchCV(
    estimator=lgb_estimator,
    param_grid=param_grid,
    scoring='neg_root_mean_squared_error',
    cv=3,
    verbose=5
)

# Timing the full operation
start_time = time.time()
grid_search_1.fit(X_train_scaled, y_train)
end_time = time.time()

# Printing results
print('Best params:', grid_search_1.best_params_)
print('Best score:', grid_search_1.best_score_)
print('Total training time (seconds):', end_time - start_time)

best_model_1 = grid_search_1.best_estimator_

best_model_1

import joblib
#joblib.dump(best_model_1, "best_lgbm_sklearn.pkl")
# Later:
best_model_1 = joblib.load("best_lgbm_sklearn.pkl")

# Predict on the validation set
y_pred_lgb = best_model_1.predict(X_val_scaled)

y_pred_lgb[:3]

import seaborn as sns
residuals = y_val - y_pred_lgb
sns.histplot(residuals)
plt.title("Validation Residuals Distribution")
plt.show()

"""# Lightlgbm RMSE"""

# Compare predictions to actual y_val labels
from sklearn.metrics import mean_squared_error
import numpy as np

val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_lgb))
print('Validation RMSE:', val_rmse)

print(y_val.min(), y_val.max(), y_val.mean(), y_val.std())

from sklearn.metrics import mean_squared_error
import numpy as np
baseline_rmse = np.sqrt(mean_squared_error(y_val, [y_train.mean()] * len(y_val)))
print('Baseline RMSE:', baseline_rmse)

import matplotlib.pyplot as plt

plt.figure(figsize=(7,7))

# True scores (as diagonal, ideal predictions)
plt.scatter(y_val, y_val, color='green', alpha=0.5, label='True Scores')

# Predicted scores (actual model predictions)
plt.scatter(y_val, y_pred_lgb, color='red', alpha=0.5, label='Predicted Scores')

plt.xlabel("True Scores")
plt.ylabel("Scores")
plt.title("True (Green Diagonal) vs. Predicted (Red) Grammar Scores (Validation)")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import lightgbm as lgb

# For LightGBM
lgb.plot_importance(grid_search_1.best_estimator_.booster_)
plt.show()

import matplotlib.pyplot as plt
import lightgbm as lgb

# This plots only the top 20 important features
lgb.plot_importance(grid_search_1.best_estimator_.booster_, max_num_features=20, figsize=(10, 6))
plt.title("Top 20 LightGBM Feature Importances")
plt.tight_layout()
plt.show()

"""****

# Ensemble RMSE
"""

#  y_pred_xgb and y_pred_lgbm are both predictions on the same validation/test set
y_pred_ensemble = (y_pred_xgb + y_pred_lgb) / 2

# Evaluate
from sklearn.metrics import mean_squared_error
ensemble_rmse = np.sqrt(mean_squared_error(y_val, y_pred_ensemble))
print("Ensemble RMSE:", ensemble_rmse)

"""# Stack Ensemble"""

from sklearn.linear_model import RidgeCV
ridge_model = RidgeCV()
ridge_model.fit(X_train_scaled, y_train)
y_pred_ridge = ridge_model.predict(X_val_scaled)

"""## Stack Ensemble RMSE"""

import numpy as np

# Meta-features: columns are predictions from your base models
meta_X = np.column_stack([y_pred_xgb, y_pred_lgb, y_pred_ridge])

# Train meta-model (can use RidgeCV or LinearRegression)
meta_model = RidgeCV()
meta_model.fit(meta_X, y_val)

# Meta-model outputs ensemble predictions
y_pred_stack = meta_model.predict(meta_X)

# Evaluate stacking ensemble RMSE
from sklearn.metrics import mean_squared_error
stack_rmse = np.sqrt(mean_squared_error(y_val, y_pred_stack))
print("Stacking Ensemble RMSE:", stack_rmse)

"""# Evaluation"""

## üìä Stage 4: Evaluation


# Predictions

# Meta-features: columns are predictions from your base models
meta_X = np.column_stack([y_pred_xgb, y_pred_lgb, y_pred_ridge])

# Train meta-model (can use RidgeCV or LinearRegression)
meta_model = RidgeCV()
meta_model.fit(meta_X, y_val)

# Meta-model outputs ensemble predictions
y_pred_stack = meta_model.predict(meta_X)
y_pred_xgb = best_model.predict(X_val_scaled)
y_pred_lgb = best_model_1.predict(X_val_scaled)

# Ensemble (average of both models)
y_pred_ensemble = (y_pred_xgb + y_pred_lgb) / 2

def evaluate_model(y_true, y_pred, model_name):
    """Calculate evaluation metrics"""
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"\n{'='*50}")
    print(f"{model_name} Performance")
    print(f"{'='*50}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE:  {mae:.4f}")
    print(f"R¬≤:   {r2:.4f}")

    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}

# Evaluate all models
metrics_xgb = evaluate_model(y_val, y_pred_xgb, "XGBoost")
metrics_lgb = evaluate_model(y_val, y_pred_lgb, "LightGBM")
metrics_ensemble = evaluate_model(y_val, y_pred_ensemble, "Ensemble")
metrics_stackensemble=evaluate_model(y_val, y_pred_stack, "Stack Ensemble")

#

# Visualization
fig, axes = plt.subplots(1, 4, figsize=(24, 5)) # Changed 1, 3 to 1, 4 to accommodate all 4 plots

for idx, (y_pred, title) in enumerate([
    (y_pred_xgb, 'XGBoost'),
    (y_pred_lgb, 'LightGBM'),
    (y_pred_ensemble, 'Ensemble'),
    (y_pred_stack,"Stack ensemble")
]):
    axes[idx].scatter(y_val, y_pred, alpha=0.6)
    axes[idx].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
    axes[idx].set_xlabel('True Score')
    axes[idx].set_ylabel('Predicted Score')
    axes[idx].set_title(f'{title} Predictions')
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""# Inference on Test Set"""

test_df=pd.read_csv("/content/drive/MyDrive/dataset/csvs/test.csv")

TRAIN_AUDIO_PATH="/content/drive/MyDrive/dataset/audios/test"

#Transcribe all training audios
print("Transcribing testing audios...")
test_transcripts = []

for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):
    audio_file = row['filename']
    audio_path = os.path.join(TRAIN_AUDIO_PATH, row["filename"] + ".wav")
  # Adjust column name
    if os.path.exists(audio_path):
        transcript = transcribe_audio(audio_path)
        test_transcripts.append(transcript)
    else:
        print(f"‚ö†Ô∏è Audio not found: {audio_path}")
        transcripts.append("")

test_df["transcripts"]=test_transcripts

test_df=pd.read_csv("/content/test_with_transcripts.csv")

test_df.head()

test_df.shape

test_df['transcript']

# Extract features from test transcripts
print("Extracting features from test transcripts...")
test_feature_list = []
test_embeddings_list = []

for transcript in tqdm(test_df['transcript']):
    features_1, embeddings_1 = extract_all_features(transcript)
    test_feature_list.append(features_1)
    test_embeddings_list.append(embeddings_1)

# Prepare test data
test_features_df = pd.DataFrame(test_feature_list)
test_embeddings_array = np.vstack(test_embeddings_list)
X_test = np.hstack([test_features_df.values, test_embeddings_array])
X_test_scaled = scaler.transform(X_test)

"""# Prediction"""

# meta ridge prediction
test_pred_ridge = ridge_model.predict(X_test_scaled)
#xgboost-model prediction
test_pred_xgb = best_model.predict(X_test_scaled)
#LightLGBM - model preediction
test_pred_lgb = best_model_1.predict(X_test_scaled)

# Prepare meta-features for training the meta-model using validation set predictions
meta_X_val = np.column_stack([y_pred_xgb, y_pred_lgb, y_pred_ridge])

# Train meta-model on validation set predictions and true validation labels
meta_model = RidgeCV()
meta_model.fit(meta_X_val, y_val)

# Prepare meta-features for prediction on the test set using test set predictions
meta_X_test = np.column_stack([test_pred_xgb, test_pred_lgb, test_pred_ridge])

# Meta-model outputs ensemble predictions for the test set
test_pred_stack = meta_model.predict(meta_X_test)

# Ensemble (average of both models) for the test set
test_pred_ensemble = (test_pred_xgb + test_pred_lgb) / 2

import pandas as pd

# Load test.csv to get filenames
test_df = pd.read_csv("/content/drive/MyDrive/dataset/csvs/test.csv")
# since the best model here is stcking ensemble so , for infrencing we are choosing that
# If you want only filename and prediction:
submission = pd.DataFrame({
    "filename": test_df["filename"],
    "prediction": test_pred_stack
})

# Save final CSV
submission.to_csv("submission.csv", index=False)

